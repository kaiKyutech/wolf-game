# 利用可能なLLM設定を名前で管理する。
# 必要に応じて項目を増減し、`model_name` で選択する。
models:
  ollama_gemma3:27b:
    provider: ollama
    model: gemma3:27b
    base_url: https://predicted-bool-grad-necklace.trycloudflare.com
    temperature: 0.2
    top_p: 0.95
    description: "Cloudflare経由のデモ用Ollama"
  ollama_gpt-oss:20b:
    provider: ollama
    model: gpt-oss:20b
    base_url: https://predicted-bool-grad-necklace.trycloudflare.com
    temperature: 0.2
    top_p: 0.95
    description: "Cloudflare経由のデモ用Ollama"
  gemini2.5-flash-lite:
    provider: gemini
    model: gemini-2.5-flash-lite
    temperature: 0.1
    max_output_tokens: 2048
    description: "高速応答向けGemini"

  openai_gpt4o_mini:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.2
    max_tokens: 2000
    description: "OpenAI GPT-4o mini"

  anthropic_claude3-haiku:
    provider: anthropic
    model: claude-3-haiku-20240307
    temperature: 0.2
    max_output_tokens: 2048
    description: "Anthropic Claude 3 Haiku"
