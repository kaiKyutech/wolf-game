# 利用可能なLLM設定を名前で管理する。
# 必要に応じて項目を増減し、`model_name` で選択する。
models:
  ollama_gemma3:27b:
    provider: ollama
    model: gemma3:27b
    base_url: https://studios-prime-homes-blair.trycloudflare.com
    temperature: 0.2
    top_p: 0.95
    description: "Cloudflare経由のデモ用Ollama"
  ollama_gpt-oss:20b:
    provider: ollama
    model: gpt-oss:20b
    base_url: https://studios-prime-homes-blair.trycloudflare.com
    temperature: 0.2
    top_p: 0.95
    description: "Cloudflare経由のデモ用Ollama"
  gemini2.5-flash-lite:
    provider: gemini
    model: gemini-2.5-flash-lite
    temperature: 0.1
    max_output_tokens: 2048
    description: "高速応答向けGemini"

  openai_gpt5nano:
    provider: openai
    model: gpt-5-nano-2025-08-07
    temperature: 0.2
    max_tokens: 2000
    description: "OpenAI GPT-5 nano"

  anthropic_claude3-haiku:
    provider: anthropic
    model: claude-3-haiku-20240307
    temperature: 0.2
    max_output_tokens: 2048
    description: "Anthropic Claude 3 Haiku"

  openai_gpt-oss-20b:
    provider: openai
    model: openai/gpt-oss-20b
    base_url: https://coast-cabin-investigate-dining.trycloudflare.com/v1
    api_key: EMPTY
    temperature: 0.2
    description: "vLLM(OpenAI互換)で提供する gpt-oss-20b"
