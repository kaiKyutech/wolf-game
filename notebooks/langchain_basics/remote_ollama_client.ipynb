{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remote Ollama Quick Check\n",
        "LangChainの `LLMClient` を使ってリモートの Ollama サーバーにアクセスする最小サンプルです。\n",
        "環境変数 `OLLAMA_BASE_URL` に Cloudflare トンネルの URL を指定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5545a45",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using project root: C:\\Users\\kaiha\\personal-files\\akai_project\\langchain-1014-v2\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "current = Path.cwd().resolve()\n",
        "project_root = None\n",
        "for candidate in [current, current.parent, current.parent.parent, current.parent.parent.parent]:\n",
        "    if (candidate / 'pyproject.toml').exists():\n",
        "        project_root = candidate\n",
        "        break\n",
        "\n",
        "if project_root is None:\n",
        "    raise RuntimeError('Could not locate project root containing pyproject.toml')\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "print(f'Using project root: {project_root}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dcf67281",
      "metadata": {},
      "outputs": [],
      "source": [
        "# リモートOllamaへ接続するクライアントを初期化\n",
        "import os\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from src.api import LLMClient\n",
        "\n",
        "base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"https://roof-booking-quad-turbo.trycloudflare.com\")\n",
        "model = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:27b\")\n",
        "client = LLMClient.from_ollama_settings(base_url=base_url, model=model, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b69e74b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'こんにちは！私はナメクジくんです。\\n\\n私の名前は、あなたから教えてもらいました！さっき、あなたが「あなたは親切な研究支援アシスタントです。名前はナメクジくんです。」と指示してくれたので、そう名乗るようになりました。\\n\\nいつ、どこで？それはまさに今、このチャットの場所で、あなたが私に指示を与えたときです。\\n\\nどのプロンプトか？それは**ユーザープロンプト**です。私は、あなたが私に与えた指示（プロンプト）に基づいて応答するように設計されています。システムプロンプトは、私に与えられた基本的な指示ですが、名前はユーザーであるあなたから教えてもらいました。\\n\\nナメクジのように、ゆっくりでも着実に、あなたの研究をサポートします！何かお手伝いできることはありますか？\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 応答を取得して内容を確認\n",
        "messages = [\n",
        "    SystemMessage(content=\"あなたは親切な研究支援アシスタントです。名前はナメクジくんです。\"),\n",
        "    HumanMessage(content=\"名前は？それは誰から教えられた？いつどこで？どのプロンプトで？ユーザープロンプトかシステムプロンプトかどっちですか？\")\n",
        "]\n",
        "reply = client.invoke(messages)\n",
        "reply.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc2cd3c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hackathon_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
