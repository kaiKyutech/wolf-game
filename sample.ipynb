{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Multi-Provider Sample\n",
        "このノートブックでは、プロジェクトのAPIを使ってOllamaとGeminiの両方に問い合わせる方法を示します。\n",
        "`config/models.yaml`でモデル設定を管理し、`create_client_from_model_name`で呼び出します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kaiha\\anaconda3\\envs\\hackathon_2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "登録済みモデル: ['ollama_gemma3-27b', 'gemini_25flash']\n"
          ]
        }
      ],
      "source": [
        "# 必要なヘルパーをインポートし、利用可能なモデル設定を表示\n",
        "from src.config import create_client_from_model_name, list_model_names\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "model_names = list_model_names()\n",
        "print('登録済みモデル:', model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollamaの例\n",
        "`models.yaml`で定義したOllama設定を利用して問い合わせます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChainを使った実験を簡単に説明しますね。LangChainは、大規模言語モデル（LLM）を様々なデータソースやツールと連携させて、より高度なアプリケーションを構築するためのフレームワークです。\\n\\n**LangChainを使った実験の基本的な流れ**\\n\\n1. **目的の定義:** まず、何を試したいのか、どんな問題を解決したいのかを明確にします。例えば、「特定のウェブサイトから情報を取得して要約する」「社内ドキュメントを検索して質問に答える」「チャットボットを作成する」などです。\\n\\n2. **コンポーネントの選択:** LangChainには様々なコンポーネント（モデル、プロンプト、チェーン、インデックス、エージェントなど）があります。目的に合わせて必要なコンポーネントを選択します。\\n    * **LLM:** OpenAIのGPT-3、GoogleのPaLMなど、使用する言語モデルを選びます。\\n    * **プロンプト:** LLMに指示を与えるためのテキストを作成します。\\n    * **チェーン:** LLMと他のコンポーネントを繋げて、一連の処理を定義します。\\n    * **インデックス:** 大量のデータを効率的に検索するための仕組みです。\\n    * **エージェント:** LLMが自律的にツールを選択し、タスクを実行する仕組みです。\\n\\n3. **コードの実装:** 選択したコンポーネントを使って、Pythonなどのプログラミング言語でコードを実装します。LangChainのドキュメントやサンプルコードを参考にすると良いでしょう。\\n\\n4. **実行と評価:** コードを実行し、結果を評価します。期待通りの結果が得られない場合は、プロンプトやコンポーネントの組み合わせを調整します。\\n\\n**簡単な実験例：ウェブサイトから情報を取得して要約する**\\n\\n1. **目的:** 特定のウェブサイト（例：Wikipediaの「LangChain」のページ）から情報を取得し、要約する。\\n2. **コンポーネント:**\\n    * **LLM:** OpenAIのGPT-3\\n    * **ドキュメントローダー:** ウェブサイトからテキストを抽出するツール\\n    * **テキストスプリッター:** 長いテキストを分割するツール\\n    * **ベクトルストア:** テキストをベクトル化して保存するデータベース\\n    * **RetrievalQAチェーン:** 質問に基づいて関連するテキストを検索し、LLMに要約させるチェーン\\n3. **コード:** LangChainのドキュメントにあるサンプルコードを参考に、上記のコンポーネントを組み合わせてコードを実装します。\\n4. **実行と評価:** コードを実行し、Wikipediaの「LangChain」のページを要約した結果を確認します。\\n\\n**LangChainのメリット**\\n\\n* **柔軟性:** 様々なLLMやデータソース、ツールと連携できる。\\n* **拡張性:** 独自のコンポーネントを追加できる。\\n* **コミュニティ:** 活発なコミュニティがあり、情報交換やサポートを受けやすい。\\n\\n**LangChainを学ぶためのリソース**\\n\\n* **公式ドキュメント:** [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction)\\n* **LangChainのチュートリアル:** [https://www.youtube.com/playlist?list=PLxCzVbH3m3zX9w9-wJgY-L-h9K_wQ-w9](https://www.youtube.com/playlist?list=PLxCzVbH3m3zX9w9-wJgY-L-h9K_wQ-w9)\\n* **LangChainのサンプルコード:** [https://github.com/langchain-ai/langchain/tree/master/examples](https://github.com/langchain-ai/langchain/tree/master/examples)\\n\\nLangChainは、LLMの可能性を最大限に引き出すための強力なツールです。ぜひ、色々な実験を通して、その機能を体験してみてください。\\n\\n何か具体的な実験のアイデアがあれば、さらに詳しく説明できますので、お気軽にご質問ください。\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ollama設定（例: ollama_default）で会話を送信\n",
        "ollama_client = create_client_from_model_name('ollama_gemma3-27b')\n",
        "ollama_messages = [\n",
        "    SystemMessage(content='あなたは誠実な日本語アシスタントです'),\n",
        "    HumanMessage(content='LangChainを使った実験を簡単に説明してください'),\n",
        "]\n",
        "ollama_reply = ollama_client.invoke(ollama_messages)\n",
        "ollama_reply.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geminiの例\n",
        "Google Geminiの設定を使って同様に問い合わせます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gemini設定（例: gemini_flash）で問い合わせ\n",
        "gemini_client = create_client_from_model_name('gemini_flash')\n",
        "gemini_messages = [\n",
        "    SystemMessage(content='You are a concise technical assistant'),\n",
        "    HumanMessage(content='Summarize how this project uses LangChain.'),\n",
        "]\n",
        "gemini_reply = gemini_client.invoke(gemini_messages)\n",
        "gemini_reply.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ストリーム受信の例\n",
        "応答をトークンごとに受け取りたい場合は`stream`を利用してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChainを使った実験を簡単に説明しますね。\n",
            "\n",
            "LangChainは、大規模言語モデル（LLM）を様々なデータソースやツールと連携させて、より高度なアプリケーションを構築するためのフレームワークです。実験と聞くと難しそうに聞こえるかもしれませんが、LangChainを使うと、LLMの可能性を試すための様々な試みを比較的簡単に実現できます。\n",
            "\n",
            "**LangChainを使った実験の例と、その目的をいくつか紹介します。**\n",
            "\n",
            "*   **質問応答システム:**\n",
            "    *   **目的:** LLMに特定のドキュメントやウェブサイトの内容に関する質問に答えさせる。\n",
            "    *   **実験内容:**\n",
            "        1.  ドキュメントを読み込み、LangChainの「Document Loader」を使ってテキストデータに変換します。\n",
            "        2.  テキストデータを分割し、LLMが処理しやすいように「Text Splitter」でチャンクに分割します。\n",
            "        3.  分割されたテキストデータを「Vectorstore」に保存し、類似度検索できるようにします。\n",
            "        4.  ユーザーからの質問を受け取り、Vectorstoreから関連性の高い情報を検索します。\n",
            "        5.  LLMに質問と関連情報を与え、回答を生成させます。\n",
            "*   **チャットボット:**\n",
            "    *   **目的:** LLMと会話できるチャットボットを作成し、その応答の質や会話の流れを評価する。\n",
            "    *   **実験内容:**\n",
            "        1.  LLMに特定の役割（例：カスタマーサポート、旅行プランナー）を与えます。\n",
            "        2.  ユーザーからの入力をLLMに送り、応答を生成させます。\n",
            "        3.  会話履歴を「Memory」に保存し、LLMが過去の会話内容を考慮できるようにします。\n",
            "        4.  応答の質、会話の自然さ、問題解決能力などを評価します。\n",
            "*   **要約:**\n",
            "    *   **目的:** 長いテキストを自動的に要約するシステムを作成し、要約の精度や情報量を評価する。\n",
            "    *   **実験内容:**\n",
            "        1.  長いテキストを読み込み、LangChainの「Document Loader」を使ってテキストデータに変換します。\n",
            "        2.  LLMにテキストを与え、要約を生成させます。\n",
            "        3.  生成された要約の長さ、内容の正確さ、重要な情報の網羅性などを評価します。\n",
            "*   **エージェント:**\n",
            "    *   **目的:** LLMに特定のタスクを実行させるために、様々なツール（検索エンジン、計算機、APIなど）を連携させる。\n",
            "    *   **実験内容:**\n",
            "        1.  LLMにタスクを与え、どのツールを使うべきか判断させます。\n",
            "        2.  LLMがツールを実行し、結果を取得します。\n",
            "        3.  LLMが結果を分析し、次のアクションを決定します。\n",
            "        4.  このプロセスを繰り返して、タスクを完了させます。\n",
            "\n",
            "**LangChainを使うメリット:**\n",
            "\n",
            "*   **柔軟性:** 様々なLLM、データソース、ツールを組み合わせて実験できます。\n",
            "*   **モジュール性:** 各コンポーネントが独立しているため、必要な部分だけをカスタマイズできます。\n",
            "*   **コミュニティ:** 活発なコミュニティがあり、情報交換やサポートを受けられます。\n",
            "\n",
            "**実験を始めるためのステップ:**\n",
            "\n",
            "1.  LangChainのインストール: `pip install langchain`\n",
            "2.  LLMのAPIキーを取得: OpenAI、Google PaLMなど、利用するLLMのAPIキーを取得します。\n",
            "3.  LangChainのドキュメントやチュートリアルを参考に、簡単な実験から始めてみましょう。\n",
            "\n",
            "LangChainは、LLMの可能性を最大限に引き出すための強力なツールです。色々な実験を通して、LLMの能力を理解し、新しいアプリケーションを開発してみてください。\n",
            "\n",
            "もし、特定の実験についてもっと詳しく知りたい場合は、お気軽にご質問ください。\n"
          ]
        }
      ],
      "source": [
        "# Ollama設定でストリーミング\n",
        "for chunk in create_client_from_model_name('ollama_gemma3-27b').stream(ollama_messages):\n",
        "    print(chunk, end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e55d76",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hackathon_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
