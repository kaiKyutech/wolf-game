{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Multi-Provider Sample\n",
        "このノートブックでは、プロジェクトのAPIを使ってOllamaとGeminiの両方に問い合わせる方法を示します。\n",
        "`config/models.yaml`でモデル設定を管理し、`create_client_from_model_name`で呼び出します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "登録済みモデル: ['ollama_gemma3-27b', 'gemini_25flash']\n"
          ]
        }
      ],
      "source": [
        "# 必要なヘルパーをインポートし、利用可能なモデル設定を表示\n",
        "from src.config import create_client_from_model_name, list_model_names\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "model_names = list_model_names()\n",
        "print('登録済みモデル:', model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollamaの例\n",
        "`models.yaml`で定義したOllama設定を利用して問い合わせます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChainを使った実験を簡単に説明しますね。\\n\\nLangChainは、大規模言語モデル（LLM）を様々なデータソースやツールと連携させて、より高度なアプリケーションを構築するためのフレームワークです。実験と聞くと難しそうに聞こえるかもしれませんが、LangChainを使うと、LLMの可能性を試すための様々な試みを比較的簡単に実現できます。\\n\\n**LangChainを使った実験の例と、その目的、簡単な流れをいくつか紹介します。**\\n\\n1. **質問応答システム:**\\n   * **目的:** 特定のドキュメント（PDF、Webサイトなど）の内容に基づいて質問に答えるシステムを構築する。\\n   * **流れ:**\\n      1. ドキュメントを読み込み、テキストに変換する。\\n      2. 変換したテキストをLangChainの「Document Loader」で読み込む。\\n      3. 読み込んだドキュメントを「Text Splitter」で分割し、LLMが処理しやすいようにする。\\n      4. 分割したテキストを「Vectorstore」に保存する（ベクトルデータベース）。\\n      5. ユーザーからの質問を受け取り、質問をベクトル化する。\\n      6. ベクトル化された質問とVectorstore内のテキストを比較し、関連性の高い情報を検索する。\\n      7. 検索された情報と質問をLLMに渡し、回答を生成する。\\n\\n2. **要約:**\\n   * **目的:** 長いテキストを要約する。\\n   * **流れ:**\\n      1. 要約したいテキストをLangChainに読み込ませる。\\n      2. LangChainの「Summarization Chain」を使ってテキストを要約する。\\n      3. 生成された要約を確認する。\\n\\n3. **チャットボット:**\\n   * **目的:** ユーザーとの対話を通じて、タスクを実行したり、情報を提供したりするチャットボットを構築する。\\n   * **流れ:**\\n      1. ユーザーからの入力を受け取る。\\n      2. 入力をLLMに渡し、応答を生成する。\\n      3. 生成された応答をユーザーに返す。\\n      4. 会話履歴を保存し、LLMに渡すことで、より文脈に沿った応答を生成する。\\n\\n4. **エージェント:**\\n   * **目的:** LLMに特定のツール（検索エンジン、計算機、APIなど）を使わせ、自律的にタスクを実行させる。\\n   * **流れ:**\\n      1. LLMにタスクを与え、どのツールを使うべきか判断させる。\\n      2. LLMが選択したツールを実行し、結果を取得する。\\n      3. 取得した結果をLLMに渡し、次のアクションを決定させる。\\n      4. このプロセスを繰り返すことで、LLMが自律的にタスクを完了する。\\n\\n**LangChainを使うメリット:**\\n\\n* **柔軟性:** 様々なデータソースやツールと連携できる。\\n* **モジュール性:** 各機能をモジュールとして提供しており、必要な機能だけを組み合わせて利用できる。\\n* **コミュニティ:** 活発なコミュニティがあり、情報交換やサポートを受けやすい。\\n\\n**実験を始めるためのリソース:**\\n\\n* **LangChain公式ドキュメント:** [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction)\\n* **LangChainチュートリアル:** [https://github.com/langchain-ai/langchain/tree/master/docs/extras/use_cases](https://github.com/langchain-ai/langchain/tree/master/docs/extras/use_cases)\\n\\nこれらの情報を参考に、ぜひLangChainを使った実験に挑戦してみてください。もし具体的な実験内容について質問があれば、お気軽にお尋ねください。\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ollama設定（例: ollama_default）で会話を送信\n",
        "ollama_client = create_client_from_model_name('ollama_gemma3-27b')\n",
        "ollama_messages = [\n",
        "    SystemMessage(content='あなたは誠実な日本語アシスタントです'),\n",
        "    HumanMessage(content='LangChainを使った実験を簡単に説明してください'),\n",
        "]\n",
        "ollama_reply = ollama_client.invoke(ollama_messages)\n",
        "ollama_reply.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geminiの例\n",
        "Google Geminiの設定を使って同様に問い合わせます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gemini設定（例: gemini_flash）で問い合わせ\n",
        "gemini_client = create_client_from_model_name('gemini_flash')\n",
        "gemini_messages = [\n",
        "    SystemMessage(content='You are a concise technical assistant'),\n",
        "    HumanMessage(content='Summarize how this project uses LangChain.'),\n",
        "]\n",
        "gemini_reply = gemini_client.invoke(gemini_messages)\n",
        "gemini_reply.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ストリーム受信の例\n",
        "応答をトークンごとに受け取りたい場合は`stream`を利用してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChainを使った実験を簡単に説明しますね。\n",
            "\n",
            "LangChainは、大規模言語モデル（LLM）を様々なデータソースやツールと連携させて、より複雑なタスクを実行するためのフレームワークです。実験は、このLangChainを使って、LLMの可能性を探ったり、特定のタスクに対する最適な構成を見つけたりすることを指します。\n",
            "\n",
            "**実験の基本的な流れ:**\n",
            "\n",
            "1. **目的の設定:** 何を試したいのか、どんな問題を解決したいのかを明確にします。例えば、「特定のドキュメントの内容を要約する」「質問応答システムを作る」「チャットボットを作る」など。\n",
            "2. **コンポーネントの選択:** LangChainには様々なコンポーネント（モデル、プロンプト、チェーン、インデックスなど）があります。目的に合わせて適切なコンポーネントを選びます。\n",
            "3. **チェーンの構築:** 選んだコンポーネントを組み合わせて、処理の流れ（チェーン）を定義します。例えば、「ドキュメントを読み込み -> 関連する情報を検索 -> LLMに質問 -> 回答を生成」のような流れです。\n",
            "4. **実行と評価:** チェーンを実行し、結果を評価します。結果が期待通りでなければ、コンポーネントの変更やチェーンの調整を行います。\n",
            "5. **繰り返し:** 上記のステップを繰り返し、より良い結果が得られるように改善していきます。\n",
            "\n",
            "**実験の例:**\n",
            "\n",
            "* **ドキュメント要約:**\n",
            "    * **目的:** 長いドキュメントを簡潔に要約する。\n",
            "    * **コンポーネント:** ドキュメントローダー、テキスト分割器、LLM、要約チェーン\n",
            "    * **実験:** 様々なLLMやプロンプトを試して、最も自然で正確な要約を生成できる組み合わせを見つける。\n",
            "* **質問応答:**\n",
            "    * **目的:** 特定のドキュメントに関する質問に答える。\n",
            "    * **コンポーネント:** ドキュメントローダー、テキスト分割器、ベクトルデータベース、LLM、質問応答チェーン\n",
            "    * **実験:** ベクトルデータベースの種類やLLMのパラメータを調整して、質問に対する回答の精度を向上させる。\n",
            "* **チャットボット:**\n",
            "    * **目的:** ユーザーとの対話を通じて、特定のタスクを実行する。\n",
            "    * **コンポーネント:** LLM、メモリ、ツール、エージェント\n",
            "    * **実験:** 異なるエージェントの種類やツールを組み合わせて、より自然で効果的な対話を実現する。\n",
            "\n",
            "**LangChainを使うメリット:**\n",
            "\n",
            "* **柔軟性:** 様々なコンポーネントを組み合わせて、独自のアプリケーションを構築できる。\n",
            "* **拡張性:** 新しいコンポーネントやツールを簡単に追加できる。\n",
            "* **コミュニティ:** 活発なコミュニティがあり、情報交換やサポートを受けやすい。\n",
            "\n",
            "LangChainは、LLMの可能性を最大限に引き出すための強力なツールです。色々な実験を通じて、LLMの活用方法を探求してみてください。\n",
            "\n",
            "より具体的な実験例や、LangChainの詳しい情報については、以下の公式ドキュメントやチュートリアルを参照してください。\n",
            "\n",
            "* **LangChain公式ドキュメント:** [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction)\n",
            "* **LangChainチュートリアル:** [https://github.com/langchain-ai/langchain/tree/master/docs/extras/use_cases](https://github.com/langchain-ai/langchain/tree/master/docs/extras/use_cases)\n",
            "\n",
            "何か他に知りたいことがあれば、お気軽にご質問ください。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ollama設定でストリーミング\n",
        "for chunk in create_client_from_model_name('ollama_gemma3-27b').stream(ollama_messages):\n",
        "    print(chunk, end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e55d76",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hackathon_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
