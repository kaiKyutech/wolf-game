{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Multi-Provider Sample\n",
        "このノートブックでは、プロジェクトのAPIを使ってOllamaとGeminiの両方に問い合わせる方法を示します。",
        "`config/models.yaml`でモデル設定を管理し、`create_client_from_model_name`で呼び出します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "source": [
        "# プロジェクトルートをパスに追加しsrcをインポート\n",
        "from pathlib import Path\n",
        "import sys\n\n",
        "project_root = Path.cwd().resolve()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "print(f'Project root: {project_root}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 必要なヘルパーをインポートし、利用可能なモデル設定を表示\n",
        "from src.config import create_client_from_model_name, list_model_names\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "model_names = list_model_names()\n",
        "print('登録済みモデル:', model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollamaの例\n",
        "`models.yaml`で定義したOllama設定を利用して問い合わせます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ollama設定（例: ollama_default）で会話を送信\n",
        "ollama_client = create_client_from_model_name('ollama_default')\n",
        "ollama_messages = [\n",
        "    SystemMessage(content='あなたは誠実な日本語アシスタントです'),\n",
        "    HumanMessage(content='LangChainを使った実験を簡単に説明してください'),\n",
        "]\n",
        "ollama_reply = ollama_client.invoke(ollama_messages)\n",
        "ollama_reply.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geminiの例\n",
        "Google Geminiの設定を使って同様に問い合わせます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Gemini設定（例: gemini_flash）で問い合わせ\n",
        "gemini_client = create_client_from_model_name('gemini_flash')\n",
        "gemini_messages = [\n",
        "    SystemMessage(content='You are a concise technical assistant'),\n",
        "    HumanMessage(content='Summarize how this project uses LangChain.'),\n",
        "]\n",
        "gemini_reply = gemini_client.invoke(gemini_messages)\n",
        "gemini_reply.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ストリーム受信の例\n",
        "応答をトークンごとに受け取りたい場合は`stream`を利用してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ollama設定でストリーミング\n",
        "for chunk in create_client_from_model_name('ollama_default').stream(ollama_messages):\n",
        "    print(chunk, end='')\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}